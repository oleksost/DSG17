{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The task it to predict material demand 3 month in advance starting from March 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.objectives import mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import LSTM, SimpleRNN, GRU,GlobalMaxPooling1D,Conv1D,MaxPooling1D, Dropout, Bidirectional\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data (load, aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/demand_anonymized_20170802.csv\", delimiter=\";\", parse_dates=[\"Month\"])\n",
    "eval_set = pd.read_csv('data/eval_correct.csv')\n",
    "series = data.groupby([\"SalOrg\", \"Material\", \"Month\"])[\"OrderQty\"].sum().reset_index()\n",
    "series = series.sort_values(by=[\"SalOrg\", \"Material\", \"Month\"])\n",
    "series.head()\n",
    "eval_comb = eval_set[['Material', 'SalOrg']]\n",
    "eval_comb = list(set([tuple(x) for x in eval_comb.values])) # gets unique combinations\n",
    "\n",
    "comb = list(itertools.product(*[eval_comb, list(series['Month'].unique())]))\n",
    "comb = [(t[0], t[1], m) for t, m in comb]\n",
    "\n",
    "series2 = pd.DataFrame(comb, columns=['Material', 'SalOrg', 'Month'])\n",
    "series2 = series2.sort_values(by=['Material', 'SalOrg', 'Month' ])\n",
    "\n",
    "series2 = series2.merge(series, on=['Month', 'Material', 'SalOrg'], how='left')\n",
    "series2 = series2.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# validation folds\n",
    "#training on the last 4 month, since othervise lags will be very sparce (go beyond the availiable data)\n",
    "def create_validation(data, date, m):\n",
    "    return data[(data[\"Month\"] >= pd.to_datetime(date) - relativedelta(months=m))&\n",
    "                (data[\"Month\"] < pd.to_datetime(date))].index, data[(data[\"Month\"] >= pd.to_datetime(date)) &\n",
    "                                                                    (data[\"Month\"] < pd.to_datetime(date) + relativedelta(months=3))].index\n",
    "\n",
    "validation_months = ['2016-08-01', '2016-09-01', '2016-10-01', '2016-11-01']\n",
    "\n",
    "#preparing validation folds\n",
    "folds = []\n",
    "for month in validation_months:\n",
    "    folds.append(create_validation(series2, month, 4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_phase = True\n",
    "num_lagged = 24\n",
    "num_predict = 3\n",
    "\n",
    "def lag_feature(df, colname, lag, num_pred):\n",
    "    cols_lagged = []\n",
    "    for i in range(lag, -1, -1):\n",
    "        col_lagged = '{}(t-{})'.format(colname, i)\n",
    "        df[col_lagged] = df.groupby(by=['SalOrg', 'Material'])[colname].shift(i)\n",
    "        cols_lagged.append(col_lagged)\n",
    "    cols_fut = []\n",
    "    for i in range(1, num_pred):\n",
    "        col_fut_name = '{}(t+{})'.format(colname, i)\n",
    "        df[col_fut_name] = df.groupby(by=['SalOrg', 'Material'])[colname].shift(-i)\n",
    "        cols_fut.append(cols_fut)\n",
    "\n",
    "    return df, cols_lagged, cols_fut\n",
    "\n",
    "series, lagged, cols_future = lag_feature(series2, 'OrderQty', num_lagged, num_predict)\n",
    "series.head()\n",
    "\n",
    "\n",
    "reframed = series[lagged + cols_future]\n",
    "reframed.dropna(inplace=True)\n",
    "\n",
    "idx = reframed.index\n",
    "values = reframed.values\n",
    "\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# fit the MinMax sacler to normalize features, to unscale later same dimentionality should be used i.e. 27 columns\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(train_X):\n",
    "    main_input = Input(shape=(train_X.shape[1], train_X.shape[2]), name='main_input')\n",
    "    #x = Conv1D(16, 3, padding='valid', activation='relu', strides=1)(main_input)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Conv1D(32, 3, padding='valid', activation='relu', strides=1)(main_input)\n",
    "    x = MaxPooling1D(pool_size = 2)(x)\n",
    "    x = GRU(32,dropout=0.2,recurrent_dropout=0.2)(x)\n",
    "    #x = keras.layers.concatenate([x, auxiliary_input])\n",
    "    output = Dense(3, activation='relu')(x)\n",
    "    model = Model(inputs=[main_input], outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='mae', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Histories(keras.callbacks.Callback):\n",
    "    def __init__(self, scaler, val_x, val_y):\n",
    "        keras.callbacks.Callback.__init__(self)\n",
    "        self.scaler = scaler\n",
    "        self.val_x = val_x\n",
    "        self.val_y = val_y\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #if epoch % 3 != 0:\n",
    "        #    return\n",
    "        y_pred = self.model.predict(self.val_x)\n",
    "        temp_x = self.val_x.reshape((self.val_x.shape[0], self.val_x.shape[1]))\n",
    "        concat = np.concatenate((temp_x,y_pred), axis=1)\n",
    "        val_unscaled = self.scaler.inverse_transform(concat)\n",
    "        y_pred = val_unscaled[:, -1 * num_predict:]\n",
    "        concat = np.concatenate((temp_x,self.val_y), axis=1)\n",
    "        val_unscaled = self.scaler.inverse_transform(concat)\n",
    "        y_true = val_unscaled[:, -1 * num_predict:]\n",
    "        print(\"Epoch ended\")\n",
    "        print(\"Test mse \", mean_absolute_error(y_true, y_pred))\n",
    "        return\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((154704, 24, 1), (154704, 3), (116028, 24, 1), (116028, 3))\n",
      "Train on 154704 samples, validate on 116028 samples\n",
      "Epoch 1/3\n",
      "154624/154704 [============================>.] - ETA: 0s - loss: 9.7602e-04Epoch ended\n",
      "15.023842377\n",
      "154704/154704 [==============================] - 162s - loss: 9.7596e-04 - val_loss: 0.0011\n",
      "Epoch 2/3\n",
      "154688/154704 [============================>.] - ETA: 0s - loss: 9.3640e-04Epoch ended\n",
      "14.9210069401\n",
      "154704/154704 [==============================] - 156s - loss: 9.3631e-04 - val_loss: 0.0011\n",
      "Epoch 3/3\n",
      "154624/154704 [============================>.] - ETA: 0s - loss: 7.7053e-04Epoch ended\n",
      "12.5129446105\n",
      "154704/154704 [==============================] - 145s - loss: 7.7052e-04 - val_loss: 9.2415e-04\n",
      "((154704, 24, 1), (154704, 3), (116028, 24, 1), (116028, 3))\n",
      "Train on 154704 samples, validate on 116028 samples\n",
      "Epoch 1/3\n",
      "154624/154704 [============================>.] - ETA: 0s - loss: 8.4428e-04Epoch ended\n",
      "13.2441935096\n",
      "154704/154704 [==============================] - 148s - loss: 8.4423e-04 - val_loss: 9.7815e-04\n",
      "Epoch 2/3\n",
      "154688/154704 [============================>.] - ETA: 0s - loss: 7.8874e-04Epoch ended\n",
      "12.6533630886\n",
      "154704/154704 [==============================] - 146s - loss: 7.8867e-04 - val_loss: 9.3452e-04\n",
      "Epoch 3/3\n",
      "154688/154704 [============================>.] - ETA: 0s - loss: 7.6807e-04Epoch ended\n",
      "12.4106181083\n",
      "154704/154704 [==============================] - 143s - loss: 7.6800e-04 - val_loss: 9.1659e-04\n",
      "((154704, 24, 1), (154704, 3), (116028, 24, 1), (116028, 3))\n",
      "Train on 154704 samples, validate on 116028 samples\n",
      "Epoch 1/3\n",
      "154688/154704 [============================>.] - ETA: 0s - loss: 8.9747e-04Epoch ended\n",
      "12.0921833909\n",
      "154704/154704 [==============================] - 142s - loss: 8.9739e-04 - val_loss: 8.9307e-04\n",
      "Epoch 2/3\n",
      "154688/154704 [============================>.] - ETA: 0s - loss: 8.3310e-04Epoch ended\n",
      "11.2188869935\n",
      "154704/154704 [==============================] - 143s - loss: 8.3302e-04 - val_loss: 8.2857e-04\n",
      "Epoch 3/3\n",
      "154624/154704 [============================>.] - ETA: 0s - loss: 8.0352e-04Epoch ended\n",
      "11.0269815921\n",
      "154704/154704 [==============================] - 141s - loss: 8.0353e-04 - val_loss: 8.1440e-04\n",
      "((154704, 24, 1), (154704, 3), (116028, 24, 1), (116028, 3))\n",
      "Train on 154704 samples, validate on 116028 samples\n",
      "Epoch 1/3\n",
      "154624/154704 [============================>.] - ETA: 0s - loss: 0.0011Epoch ended\n",
      "13.2448599247\n",
      "154704/154704 [==============================] - 133s - loss: 0.0011 - val_loss: 9.7820e-04\n",
      "Epoch 2/3\n",
      "154688/154704 [============================>.] - ETA: 0s - loss: 0.0011Epoch ended\n",
      "13.140911306\n",
      "154704/154704 [==============================] - 140s - loss: 0.0011 - val_loss: 9.7053e-04\n",
      "Epoch 3/3\n",
      "154624/154704 [============================>.] - ETA: 0s - loss: 0.0010Epoch ended\n",
      "13.0032130266\n",
      "154704/154704 [==============================] - 133s - loss: 0.0010 - val_loss: 9.6036e-04\n"
     ]
    }
   ],
   "source": [
    "num_epochs=3 # change to 50\n",
    "\n",
    "for fold in folds:\n",
    "    reframed.index = idx\n",
    "    train = reframed.loc[fold[0]]\n",
    "    test = reframed.loc[fold[1]]\n",
    "    scaled_train = scaler.transform(train)\n",
    "    scaled_val = scaler.transform(test)\n",
    "    \n",
    "    train_X, train_y = scaled_train[:, :-1 * num_predict], scaled_train[:, -1 * num_predict:]\n",
    "    test_X, test_y = scaled_val[:, :-1 * num_predict], scaled_val[:, -1 * num_predict:]\n",
    "    \n",
    "    train_X = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))\n",
    "    test_X = test_X.reshape((test_X.shape[0],test_X.shape[1], 1))\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  min_delta=0,\n",
    "                                  patience=2,\n",
    "                                  verbose=0, mode='auto')\n",
    "    history = Histories(scaler, test_X, test_y)\n",
    "    model = create_model(train_X)\n",
    "    # fit network\n",
    "    \n",
    "    if train_phase:\n",
    "        model.fit(train_X, train_y, epochs = nun_epochs, batch_size = 64, validation_data = (test_X, test_y), verbose = 1, shuffle = False, callbacks = [early_stop, history])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the damend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ind = series[series[\"Month\"]==pd.to_datetime('2017-03-01')].index #need to predict 3 month ahead starting Marh 2017\n",
    "# in order to scale the data the same dimentionality as before is needed, therefore the cols_future are also taken\n",
    "test_x = series.loc[test_ind][lagged + cols_future]\n",
    "test_x = test_x.fillna(0)\n",
    "values = test_x.values\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "values = scaler.transform(values)\n",
    "values = values.reshape((values.shape[0], values.shape[1], 1))\n",
    "pred_X = values[:, :-1 * num_predict]\n",
    "test_prediction = model.predict(pred_X)\n",
    "\n",
    "temp_x = pred_X.reshape((pred_X.shape[0], pred_X.shape[1]))\n",
    "\n",
    "#concatenate with features again in order to unscale the predictions\n",
    "concat = np.concatenate((temp_x,test_prediction), axis=1)\n",
    "val_unscaled = scaler.inverse_transform(concat)\n",
    "\n",
    "#get the unscaled predictions\n",
    "y_pred = val_unscaled[:, -1 * num_predict:]\n",
    "\n",
    "tst = series.loc[test_ind].copy()\n",
    "tst['OrderQty(t+1)'] = y_pred.T[0]\n",
    "tst['OrderQty(t+2)'] = y_pred.T[1]\n",
    "tst['OrderQty(t+3)'] = y_pred.T[2]\n",
    "\n",
    "ml = pd.melt(tst, id_vars=['Material', 'SalOrg'], value_vars=['OrderQty(t+1)', 'OrderQty(t+2)','OrderQty(t+3)'])\n",
    "ml['date'] = ml.variable.replace({'OrderQty(t+1)':'2017-04', 'OrderQty(t+2)':'2017-05', 'OrderQty(t+3)':'2017-06'})\n",
    "del ml['variable']\n",
    "result = eval_set.merge(ml, on=['Material', 'SalOrg', 'date'])\n",
    "result['demand'] = result['value']\n",
    "result[['ID', 'demand']].to_csv('rnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
