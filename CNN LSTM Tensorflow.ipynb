{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## The following code is used for a quick and dirty Tensorflow experimental set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Cell types can be tested, drop out ratios etc. Tensorboard can be used to track the changes during the training. Also the cell hidden state and and current cell states can be better tracked and manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.objectives import mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from keras.models import Sequential,Model\n",
    "#from keras.layers import Input, Dense\n",
    "#from keras.layers import LSTM, SimpleRNN, GRU,GlobalMaxPooling1D,Conv1D,MaxPooling1D, Dropout, Bidirectional\n",
    "#from keras import optimizers\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/demand_anonymized_20170802.csv\", delimiter=\";\", parse_dates=[\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_set = pd.read_csv('data/eval_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalOrg</th>\n",
       "      <th>Material</th>\n",
       "      <th>Month</th>\n",
       "      <th>OrderQty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97LK</td>\n",
       "      <td>00IYcj</td>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97LK</td>\n",
       "      <td>00IYcj</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97LK</td>\n",
       "      <td>00IYcj</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97LK</td>\n",
       "      <td>00IYcj</td>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97LK</td>\n",
       "      <td>00IYcj</td>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SalOrg Material      Month  OrderQty\n",
       "0   97LK   00IYcj 2012-05-01         2\n",
       "1   97LK   00IYcj 2012-06-01        13\n",
       "2   97LK   00IYcj 2012-07-01         1\n",
       "3   97LK   00IYcj 2012-09-01        30\n",
       "4   97LK   00IYcj 2012-11-01         1"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = data.groupby([\"SalOrg\", \"Material\", \"Month\"])[\"OrderQty\"].sum().reset_index()\n",
    "series = series.sort_values(by=[\"SalOrg\", \"Material\", \"Month\"])\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "eval_comb = eval_set[['Material', 'SalOrg']]\n",
    "eval_comb = list(set([tuple(x) for x in eval_comb.values])) # gets unique combinations\n",
    "comb = list(itertools.product(*[eval_comb, list(series['Month'].unique())]))\n",
    "comb = [(t[0], t[1], m) for t, m in comb]\n",
    "series2 = pd.DataFrame(comb, columns=['Material', 'SalOrg', 'Month'])\n",
    "series2 = series2.sort_values(by=['Material', 'SalOrg', 'Month' ])\n",
    "series2 = series2.merge(series, on=['Month', 'Material', 'SalOrg'], how='left')\n",
    "series2 = series2.fillna(0)\n",
    "\n",
    "\n",
    "# validation folds\n",
    "#training on the last 4 month, since othervise lags will be very sparce (go beyond the availiable data)\n",
    "def create_validation(data, date, m):\n",
    "    return data[(data[\"Month\"] >= pd.to_datetime(date) - relativedelta(months=m))&\n",
    "                (data[\"Month\"] < pd.to_datetime(date))].index, data[(data[\"Month\"] >= pd.to_datetime(date)) &\n",
    "                                                                    (data[\"Month\"] < pd.to_datetime(date) + relativedelta(months=3))].index\n",
    "\n",
    "validation_months = ['2016-08-01', '2016-09-01', '2016-10-01', '2016-11-01']\n",
    "\n",
    "#preparing validation folds\n",
    "folds = []\n",
    "for month in validation_months:\n",
    "    folds.append(create_validation(series2, month, 4))\n",
    "    \n",
    "num_lagged = 24\n",
    "num_predict = 3\n",
    "def lag_feature(df, colname, lag, num_pred):\n",
    "    cols_lagged = []\n",
    "    for i in range(lag, -1, -1):\n",
    "        col_lagged = '{}(t-{})'.format(colname, i)\n",
    "        df[col_lagged] = df.groupby(by=['SalOrg', 'Material'])[colname].shift(i)\n",
    "        cols_lagged.append(col_lagged)\n",
    "    cols_fut = []\n",
    "    for i in range(1, num_pred):\n",
    "        col_fut_name = '{}(t+{})'.format(colname, i)\n",
    "        df[col_fut_name] = df.groupby(by=['SalOrg', 'Material'])[colname].shift(-i)\n",
    "        cols_fut.append(col_fut_name)\n",
    "\n",
    "    return df, cols_lagged, cols_fut\n",
    "\n",
    "series, lagged, cols_future = lag_feature(series2, 'OrderQty', num_lagged, num_predict)\n",
    "\n",
    "\n",
    "\n",
    "reframed = series[lagged + cols_future]\n",
    "reframed.dropna(inplace=True)\n",
    "idx = reframed.index\n",
    "values = reframed.values\n",
    "values = values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# fit the MinMax sacler to normalize features, to unscale later same dimentionality should be used i.e. 27 columns\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaler = scaler.fit(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_train = scaler.fit(values[:,:-3])\n",
    "scaler_valid = scaler.fit( np.reshape(values[:, -3], [-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold=folds[0]\n",
    "train = reframed.loc[fold[0]] \n",
    "test = reframed.loc[fold[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.iloc[:,:-3] = scaler_train.transform(train.iloc[:,:-3])\n",
    "train.iloc[:,-3] = scaler_valid.transform( np.reshape(np.array(train.iloc[:,-3]), [-1,1]) )\n",
    "\n",
    "test.iloc[:,:-3] = scaler_train.transform(test.iloc[:,:-3])\n",
    "test.iloc[:,-3] = scaler_valid.transform(np.reshape(np.array(test.iloc[:,-3]), [-1,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scaled_train = scaler.transform(train)\n",
    "#scaled_val = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_y = train.iloc[:, :-1 * num_predict], train.iloc[:, -1 * num_predict:]\n",
    "test_X, test_y = test.iloc[:, :-1 * num_predict], test.iloc[:, -1 * num_predict:]\n",
    "train_y_ts, test_y_ts = train_X.iloc[:,1::1], test_X.iloc[:,1::1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y_ts = pd.concat([pd.DataFrame(train_y_ts),pd.DataFrame(train_y.iloc[:,0])], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y_ts = pd.concat([pd.DataFrame(test_y_ts), pd.DataFrame(test_y.iloc[:,0])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "            act = tf.matmul(input, w) + b\n",
    "            tf.summary.histogram(\"weights\", w)\n",
    "            tf.summary.histogram(\"biases\", b)\n",
    "            tf.summary.histogram(\"activations\", act)\n",
    "            return act, w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "  with tf.name_scope(name):\n",
    "    kernel = tf.Variable(tf.truncated_normal([1, 3, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "    conv = tf.nn.conv2d(input, kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    act = tf.nn.relu(conv + b)\n",
    "    #tf.summary.histogram(\"weights\", kernel)\n",
    "    #tf.summary.histogram(\"biases\", b)\n",
    "    #tf.summary.histogram(\"activations\", act)\n",
    "    #return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "LOGDIR = \"/Users/oleksiyostapenko/Documents/DSG/Demand Prediction/tmp/logs\"\n",
    "learning_rate = 1E-5\n",
    "\n",
    "state_size = 10\n",
    "num_layers = 1\n",
    " \n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "#K.set_session(sess)\n",
    "# Setup placeholders, and reshape the data\n",
    "x_train = tf.placeholder(tf.float32, shape=[None, 24], name=\"x\")\n",
    "x_image = tf.reshape(x_train, [-1, 1, 24, 1])\n",
    "\n",
    "#x_image = tf.reshape(x_train, [-1, 2, 12]) # -1 stays for readjust the shape, so it would become the batch size\n",
    "#tf.summary.image('input', x_image, 3)\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, 24], name=\"labels\")\n",
    "#valid_dataset = tf.constant(dtype=tf.float32, value=X_test_fet.as_matrix())\n",
    "#y_test = tf.constant(dtype=tf.float32, value=labels_test.as_matrix())\n",
    "\n",
    "#init_state = tf.placeholder(tf.float32, [None, state_size])\n",
    "\n",
    "########## Conv Layer first ##########\n",
    "\n",
    "conv_act = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "#conv1 = Conv1D(32, 3, padding='valid', activation='relu', strides=1)(x)\n",
    "#conv_mp = tf.nn.max_pool(conv_act, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding=\"SAME\")\n",
    "con_out_size = 24*1*32\n",
    "flattened = tf.reshape(conv_act, [-1, con_out_size])\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "cell_state = tf.placeholder(tf.float32, [num_layers,  None, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, [num_layers,  None, state_size])\n",
    "\n",
    "#multilayered lstn\n",
    "#state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(cell_state[idx], hidden_state[idx]) # here once for \n",
    "     for idx in range(num_layers)]\n",
    ")\n",
    "\n",
    "#inputs_series = tf.unstack(flattened, axis=0)\n",
    "\n",
    "#W1\n",
    "#b1 - are in the tf cell\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, 1),dtype=tf.float32)\n",
    "#b2 = tf.Variable(np.zeros((1,1)), dtype=tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "#stacked_rnn = []\n",
    "#for _ in range(num_layers):\n",
    "#    stacked_rnn.append(tf.contrib.rnn.LSTMCell(state_size, state_is_tuple=True))\n",
    "\n",
    "#cellLSTM = tf.contrib.rnn.LSTMCell(state_size, state_is_tuple=True)\n",
    "\n",
    "cells = []\n",
    "for _ in range(num_layers):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "        cells.append(cell)\n",
    "        \n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "flattened2 = tf.reshape(flattened, [-1, 24, 32])\n",
    "#tf.expand_dims(flattened2, -1)\n",
    "\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, flattened2, initial_state=rnn_tuple_state)\n",
    "states_series = tf.reshape(states_series, [-1, state_size])\n",
    "\n",
    "logits = tf.matmul(states_series, W2) + b2 \n",
    "logits_per_period = tf.reshape(logits, [-1, 24])\n",
    "labels = tf.reshape(y, [-1])\n",
    "\n",
    "\n",
    "logits_series = tf.unstack(logits_per_period, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"objective\"):   \n",
    "        #loss is the mean absolute error\n",
    "        losses = tf.abs(tf.subtract(logits,labels))\n",
    "        total_loss = tf.reduce_mean(losses)\n",
    "        tf.summary.scalar(\"total_loss\", total_loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "        #train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "        train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "        #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(objective)\n",
    "        \n",
    "with tf.name_scope(\"mae\"):\n",
    "        #correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        mae = tf.metrics.mean_absolute_error(tf.reshape(logits_series,[24,-1])[23],y[:,23])\n",
    "        #tf.summary.scalar(\"mae\", mae)\n",
    "\n",
    "summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    #embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    #assignment = embedding.assign(embedding_input)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "sess.run(init_g)\n",
    "sess.run(init_l)\n",
    "\n",
    "\n",
    "writer = tf.summary.FileWriter(LOGDIR)\n",
    "writer.add_graph(sess.graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "_current_cell_state = np.zeros((num_layers, batch_size, state_size))\n",
    "_current_hidden_state = np.zeros((num_layers, batch_size, state_size))\n",
    "        \n",
    "last = i*batch_size\n",
    "point_X = train_X.iloc[last:last+batch_size,:].as_matrix()\n",
    "y_label = train_y_ts.iloc[last:last+batch_size,:]\n",
    "\n",
    "ll = sess.run( logits_series, feed_dict={x_train: point_X, y: y_label,\n",
    "                                                                   cell_state: _current_cell_state,\n",
    "                                                                   hidden_state: _current_hidden_state})\n",
    "#ls = sess.run(logits_series, feed_dict={x_train: point_X, y: y_label, cell_state: _current_cell_state, hidden_state: _current_hidden_state})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "('Step ', 0, 'MAE: ', (0.18970072, 0.18969749))\n",
      "('Step ', 500, 'MAE: ', (0.18832652, 0.18832393))\n",
      "('Step ', 1000, 'MAE: ', (0.18701597, 0.18701315))\n",
      "('Step ', 1500, 'MAE: ', (0.18577777, 0.18577604))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index([0], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-3ad90635e004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m#incorporating prediction for the month 2 into the test data to predict month 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtrain_y_ts_month_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_ts_month_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtrain_y_ts_month_3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y_ts_month_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg_ser_month_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     lg_ser_month_3 = sess.run( logits_series, feed_dict={x_train: train_y_ts_month_3.as_matrix(), y: test_y_ts, \n",
      "\u001b[0;32m/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4667\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4668\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 4669\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   4670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4671\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4682\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   4683\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4684\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   4685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                          copy=copy, indicator=indicator)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 575\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   4699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4700\u001b[0m             raise ValueError('columns overlap but no suffix specified: %s' %\n\u001b[0;32m-> 4701\u001b[0;31m                              to_rename)\n\u001b[0m\u001b[1;32m   4702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4703\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index([0], dtype='object')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#_current_cell_state = np.zeros((batch_size, state_size))  \n",
    "#_current_hidden_state = np.zeros((batch_size, state_size))           \n",
    "\n",
    "num_of_batches = train_X.shape[0]/batch_size\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    _current_cell_state = np.zeros((num_layers, batch_size, state_size))\n",
    "    _current_hidden_state = np.zeros((num_layers, batch_size, state_size))\n",
    "    print \"epoch\", epoch_idx\n",
    "    for i in range(num_of_batches):\n",
    "        #_init_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "        _current_cell_state = np.zeros((num_layers, batch_size, state_size))\n",
    "        _current_hidden_state = np.zeros((num_layers, batch_size, state_size))\n",
    "        \n",
    "        last = i*batch_size\n",
    "        point_X = train_X.iloc[last:last+batch_size,:].as_matrix()\n",
    "        y_label = train_y_ts.iloc[last:last+batch_size,:]\n",
    "        \n",
    "        _total_loss, _train_step, _current_state, error, summary = sess.run(\n",
    "                [total_loss, train_step, current_state, mae, summ], feed_dict={x_train: point_X, y: y_label,\n",
    "                                                                   cell_state: _current_cell_state,\n",
    "                                                                   hidden_state: _current_hidden_state})\n",
    "        writer.add_summary(summary, i+num_of_batches*epoch_idx)\n",
    "        \n",
    "        #_current_cell_state = _current_state.c\n",
    "        #_current_hidden_state = _current_state.h\n",
    "            \n",
    "            \n",
    "        if i % 500 == 0:\n",
    "            #MAE only 1 month ahead\n",
    "            print(\"Step \",i, \"MAE: \", error)\n",
    "    \n",
    "    \n",
    "    _current_cell_state = np.zeros((num_layers, test_y_ts.shape[0], state_size))\n",
    "    _current_hidden_state = np.zeros((num_layers, test_y_ts.shape[0], state_size))\n",
    "    #_init_state = np.zeros((num_layers, 2, test_y_ts.shape[0], state_size))d\n",
    "    \n",
    "    lg_ser_month_1 = sess.run( logits_series, feed_dict={x_train: test_X.as_matrix(), y: test_y_ts, \n",
    "                                              cell_state: _current_cell_state, hidden_state: _current_hidden_state})\n",
    "    \n",
    "    #incorporating prediction for the month 1 into the test data to predict month 2\n",
    "    # snce we are predicting, the labels dont play any role now\n",
    "    train_y_ts_month_2 = test_X.iloc[:,1:].as_matrix\n",
    "    train_y_ts_month_2=train_y_ts_month_2.join(pd.DataFrame(lg_ser_month_1[-1]))\n",
    "    \n",
    "    lg_ser_month_2 = sess.run( logits_series, feed_dict={x_train: train_y_ts_month_2.as_matrix(), y: test_y_ts, \n",
    "                                              cell_state: _current_cell_state,hidden_state: _current_hidden_state})\n",
    "    \n",
    "    #incorporating prediction for the month 2 into the test data to predict month 3\n",
    "    train_y_ts_month_3 = train_y_ts_month_2.iloc[:,1:]\n",
    "    train_y_ts_month_3[:,23]=lg_ser_month_2[-1]\n",
    "    \n",
    "    lg_ser_month_3 = sess.run( logits_series, feed_dict={x_train: train_y_ts_month_3.as_matrix(), y: test_y_ts, \n",
    "                                              cell_state: _current_cell_state,hidden_state: _current_hidden_state})\n",
    "    \n",
    "    pred = pd.concat([pd.DataFrame(lg_ser_month_1[-1]), pd.DataFrame(lg_ser_month_2[-1]), pd.DataFrame(lg_ser_month_3[-1])], axis=1).as_matrix()\n",
    "    pred = np.reshape(pred, (test_y_ts.shape[0],1))\n",
    "    \n",
    "    y_real = np.reshape(test_y.as_matrix(), ((test_y_ts.shape[0],1))) \n",
    "    \n",
    "    Epoch_Error = mean_absolute_error(y_pred=scaler_valid.inverse_transform(pred), y_true=scaler_valid.inverse_transform(y_real))\n",
    "    \n",
    "    print(\"Error \",Epoch_Error)\n",
    "            \n",
    "            #_current_cell_state = _current_state.c\n",
    "            #_current_hidden_state = _current_state.h\n",
    "            \n",
    "            \n",
    "        #sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "        #saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_y_ts_month_2=train_y_ts_month_2.join(pd.DataFrame(lg_ser_month_1[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y_ts_month_2 = test_X.iloc[:,1:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-344-68ee78358f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_ts_month_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg_ser_month_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/oleksiyostapenko/tensorflow/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5001\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5002\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5003\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "a = np.append(train_y_ts_month_2, lg_ser_month_2[-1], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2784672,)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
